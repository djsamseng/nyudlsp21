{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtcBjMq7YV3f"
   },
   "source": [
    "# Homework 2 - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn-cOk1iZTtR"
   },
   "source": [
    "In this part of the homework we are going to work with Recurrent Neural Networks, in particular GRU. One of the greatest things that Recurrent Neural Networks can do when working with sequences is retaining data from several timesteps in the past. We are going to explore that property by constructing an 'echo' Recurrent Neural Network.\n",
    "\n",
    "The goal here is to make a model that given a sequence of letters or digits will output that same sequence, but with a certain delay. Let's say the input is a string 'abacaba', we want the model to not output anything for 3 steps (delay length), and then output the original string step by step, except the last 3 characters. So, target output is then 'XXXabac', where 'X' is empty output.\n",
    "\n",
    "This is similar to [this notebook](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/09-echo_data.ipynb) (which you should refer to when doing this assignment), except we're working not with a binary string, but with a sequence of integers between 0 and some N. In our case N is 26, which is the number of letters in the alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npLlE973as6x"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Let's implement the dataset. In our case, the data is basically infinite, as we can always generate more examples on the fly, so don't need to load anything from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0.]]) tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "class EchoData():\n",
    "\n",
    "    def __init__(self, series_length=40000, batch_size=32,\n",
    "                 echo_step=3, truncated_length=10, seed=None, num_classes=2):\n",
    "        self.num_classes=num_classes\n",
    "        self.series_length = series_length\n",
    "        self.truncated_length = truncated_length\n",
    "        self.n_batches = series_length//truncated_length\n",
    "\n",
    "        self.echo_step = echo_step\n",
    "        self.batch_size = batch_size\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.x_batch = None\n",
    "        self.y_batch = None\n",
    "        self.x_chunks = []\n",
    "        self.y_chunks = []\n",
    "        self.generate_new_series()\n",
    "        self.prepare_batches()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index == 0:\n",
    "            self.generate_new_series()\n",
    "            self.prepare_batches()\n",
    "        x = self.x_chunks[index].astype(np.float32)\n",
    "        y = self.y_chunks[index].astype(np.float32)\n",
    "        return (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def generate_new_series(self):\n",
    "        x = np.random.choice(\n",
    "            self.num_classes,\n",
    "            size=(self.batch_size, self.series_length))\n",
    "        y = np.roll(x, self.echo_step, axis=1)\n",
    "        y[:, 0:self.echo_step] = 0\n",
    "        self.x_batch = x\n",
    "        self.y_batch = y\n",
    "\n",
    "    def prepare_batches(self):\n",
    "        x = np.expand_dims(self.x_batch, axis=-1)\n",
    "        y = np.expand_dims(self.y_batch, axis=-1)\n",
    "        self.x_chunks = np.split(x, self.n_batches, axis=1)\n",
    "        self.y_chunks = np.split(y, self.n_batches, axis=1)\n",
    "        def turn_into_onehot2(x):\n",
    "            x_new = np.zeros((x.shape[0], x.shape[1], self.num_classes))\n",
    "            for i in range(x.shape[0]):\n",
    "                for j in range(x.shape[1]):\n",
    "                    x_new[i,j, x[i,j,0]] = 1\n",
    "            return x_new\n",
    "        for i in range(len(self.x_chunks)):\n",
    "            self.x_chunks[i] = turn_into_onehot2(self.x_chunks[i])\n",
    "            self.y_chunks[i] = turn_into_onehot2(self.y_chunks[i])\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(1);\n",
    "\n",
    "batch_size = 5\n",
    "BATCH_SIZE = batch_size\n",
    "echo_step = 4\n",
    "DELAY = echo_step\n",
    "series_length = 20_000\n",
    "BPTT_T = 20\n",
    "total_values_in_one_chunck = batch_size * BPTT_T\n",
    "\n",
    "NUM_CLASSES = 27\n",
    "train_data = EchoData(\n",
    "    echo_step=echo_step,\n",
    "    batch_size=batch_size,\n",
    "    series_length=series_length,\n",
    "    truncated_length=BPTT_T,\n",
    "    num_classes=NUM_CLASSES\n",
    "    #total_values_in_one_chunck = batch_size * BPTT_T,\n",
    ")\n",
    "\n",
    "train_size = len(train_data)\n",
    "\n",
    "test_data = EchoData(\n",
    "    echo_step=echo_step,\n",
    "    batch_size=batch_size,\n",
    "    series_length=series_length,\n",
    "    truncated_length=BPTT_T,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "test_size = len(test_data)\n",
    "train_data.generate_new_series()\n",
    "\n",
    "def turn_into_onehot(x, num_classes):\n",
    "    x_new = torch.zeros((x.shape[0], num_classes,))\n",
    "    for i in range(x.shape[0]):\n",
    "        x_new[i, x[i]] = 1\n",
    "    return x_new\n",
    "\n",
    "class EchoDataset(torch.utils.data.IterableDataset):\n",
    "\n",
    "  def __init__(self, delay=4, seq_length=15, size=1000):\n",
    "    self.delay = delay\n",
    "    self.seq_length = seq_length\n",
    "    self.size = size\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.size\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\" Iterable dataset doesn't have to implement __getitem__.\n",
    "        Instead, we only need to implement __iter__ to return\n",
    "        an iterator (or generator).\n",
    "    \"\"\"\n",
    "    for _ in range(self.size):\n",
    "      seq = torch.tensor([random.choice(range(1, NUM_CLASSES)) for i in range(self.seq_length)], dtype=torch.int64)\n",
    "      result = torch.cat((torch.zeros(self.delay), seq[:self.seq_length - self.delay])).type(torch.int64)\n",
    "      seq = turn_into_onehot(seq, NUM_CLASSES)\n",
    "      result = turn_into_onehot(result, NUM_CLASSES)\n",
    "      yield seq, result\n",
    "    \n",
    "      result = torch.roll(seq, shifts=self.delay, dims=0)#\n",
    "      result[:self.delay,:] = 0\n",
    "      result[:self.delay,0] = 1\n",
    "      yield seq, result\n",
    "\n",
    "ds = EchoDataset(delay=DELAY, size=series_length)\n",
    "\n",
    "echo_dataloader = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "DX, DY = next(iter(echo_dataloader))\n",
    "print(DX[0], DY[0])\n",
    "print(test_data[1][0][0])\n",
    "#print(\"========\")\n",
    "print(test_data[1][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "epoch: 0 batch 0 loss: 0.6907798051834106 correct%: 0.0\n",
      "epoch: 0 batch 100 loss: 0.14423011243343353 correct%: 0.0\n",
      "epoch: 0 batch 200 loss: 0.14570052921772003 correct%: 0.0\n",
      "epoch: 0 batch 300 loss: 0.1468046009540558 correct%: 0.0\n",
      "epoch: 0 batch 400 loss: 0.14651091396808624 correct%: 0.0\n",
      "epoch: 0 batch 500 loss: 0.14730216562747955 correct%: 0.0\n",
      "epoch: 0 batch 600 loss: 0.14499692618846893 correct%: 0.0\n",
      "epoch: 0 batch 700 loss: 0.14782045781612396 correct%: 0.0\n",
      "epoch: 0 batch 800 loss: 0.14268794655799866 correct%: 0.0\n",
      "epoch: 0 batch 900 loss: 0.14636695384979248 correct%: 0.0\n",
      "epoch: 0 batch 1000 loss: 0.14640885591506958 correct%: 0.0\n",
      "epoch: 0 batch 1100 loss: 0.14622709155082703 correct%: 0.0\n",
      "epoch: 0 batch 1200 loss: 0.14610882103443146 correct%: 0.0\n",
      "epoch: 0 batch 1300 loss: 0.14603480696678162 correct%: 0.0\n",
      "epoch: 0 batch 1400 loss: 0.14889195561408997 correct%: 0.0\n",
      "epoch: 0 batch 1500 loss: 0.14581379294395447 correct%: 0.0\n",
      "epoch: 0 batch 1600 loss: 0.14485913515090942 correct%: 0.0\n",
      "epoch: 0 batch 1700 loss: 0.14558008313179016 correct%: 0.0\n",
      "epoch: 0 batch 1800 loss: 0.14574649930000305 correct%: 0.0\n",
      "epoch: 0 batch 1900 loss: 0.14459668099880219 correct%: 0.0\n",
      "epoch: 0 batch 2000 loss: 0.14481207728385925 correct%: 0.0\n",
      "epoch: 0 batch 2100 loss: 0.14164520800113678 correct%: 0.0\n",
      "epoch: 0 batch 2200 loss: 0.1421785205602646 correct%: 0.0\n",
      "epoch: 0 batch 2300 loss: 0.13915422558784485 correct%: 0.0\n",
      "epoch: 0 batch 2400 loss: 0.13534995913505554 correct%: 0.0\n",
      "epoch: 0 batch 2500 loss: 0.13628050684928894 correct%: 0.0\n",
      "epoch: 0 batch 2600 loss: 0.13756121695041656 correct%: 0.0\n",
      "epoch: 0 batch 2700 loss: 0.135528102517128 correct%: 0.0\n",
      "epoch: 0 batch 2800 loss: 0.13314314186573029 correct%: 0.0\n",
      "epoch: 0 batch 2900 loss: 0.1287211924791336 correct%: 0.0\n",
      "epoch: 0 batch 3000 loss: 0.13265769183635712 correct%: 0.0\n",
      "epoch: 0 batch 3100 loss: 0.13215231895446777 correct%: 0.0\n",
      "epoch: 0 batch 3200 loss: 0.12683343887329102 correct%: 0.0\n",
      "epoch: 0 batch 3300 loss: 0.13023577630519867 correct%: 0.0\n",
      "epoch: 0 batch 3400 loss: 0.12013973295688629 correct%: 0.0\n",
      "epoch: 0 batch 3500 loss: 0.12924997508525848 correct%: 0.0\n",
      "epoch: 0 batch 3600 loss: 0.12293250858783722 correct%: 0.0\n",
      "epoch: 0 batch 3700 loss: 0.12307880073785782 correct%: 0.0\n",
      "epoch: 0 batch 3800 loss: 0.12168648093938828 correct%: 0.0\n",
      "epoch: 0 batch 3900 loss: 0.12337899953126907 correct%: 0.0\n",
      "epoch: 0 batch 4000 loss: 0.12234024703502655 correct%: 0.0\n",
      "epoch: 0 batch 4100 loss: 0.12129689007997513 correct%: 0.0\n",
      "epoch: 0 batch 4200 loss: 0.11940637975931168 correct%: 0.0\n",
      "epoch: 0 batch 4300 loss: 0.11465592682361603 correct%: 0.0\n",
      "epoch: 0 batch 4400 loss: 0.10563633590936661 correct%: 0.0\n",
      "epoch: 0 batch 4500 loss: 0.11756133288145065 correct%: 0.0\n",
      "epoch: 0 batch 4600 loss: 0.10498692095279694 correct%: 0.0\n",
      "epoch: 0 batch 4700 loss: 0.10505805164575577 correct%: 0.0\n",
      "epoch: 0 batch 4800 loss: 0.10550150275230408 correct%: 0.0\n",
      "epoch: 0 batch 4900 loss: 0.10983482748270035 correct%: 0.0\n",
      "epoch: 0 batch 5000 loss: 0.10536348074674606 correct%: 0.0\n",
      "epoch: 0 batch 5100 loss: 0.10430918633937836 correct%: 0.0\n",
      "epoch: 0 batch 5200 loss: 0.09756141901016235 correct%: 0.0\n",
      "epoch: 0 batch 5300 loss: 0.1016053557395935 correct%: 0.0\n",
      "epoch: 0 batch 5400 loss: 0.09508541971445084 correct%: 0.0\n",
      "epoch: 0 batch 5500 loss: 0.09558263421058655 correct%: 0.0\n",
      "epoch: 0 batch 5600 loss: 0.09362141788005829 correct%: 0.0\n",
      "epoch: 0 batch 5700 loss: 0.08628131449222565 correct%: 0.0\n",
      "epoch: 0 batch 5800 loss: 0.0837007686495781 correct%: 0.0\n",
      "epoch: 0 batch 5900 loss: 0.07766535878181458 correct%: 0.0\n",
      "epoch: 0 batch 6000 loss: 0.07678277045488358 correct%: 0.0\n",
      "epoch: 0 batch 6100 loss: 0.0726805031299591 correct%: 0.0\n",
      "epoch: 0 batch 6200 loss: 0.0763254463672638 correct%: 0.0\n",
      "epoch: 0 batch 6300 loss: 0.07629984617233276 correct%: 0.0\n",
      "epoch: 0 batch 6400 loss: 0.07664311677217484 correct%: 0.0\n",
      "epoch: 0 batch 6500 loss: 0.07862555235624313 correct%: 0.0\n",
      "epoch: 0 batch 6600 loss: 0.07905327528715134 correct%: 0.2\n",
      "epoch: 0 batch 6700 loss: 0.07261931151151657 correct%: 0.2\n",
      "epoch: 0 batch 6800 loss: 0.06722477078437805 correct%: 0.6\n",
      "epoch: 0 batch 6900 loss: 0.06389035284519196 correct%: 1.8\n",
      "epoch: 0 batch 7000 loss: 0.06524012982845306 correct%: 2.6\n",
      "epoch: 0 batch 7100 loss: 0.06635266542434692 correct%: 1.8\n",
      "epoch: 0 batch 7200 loss: 0.06447968631982803 correct%: 6.4\n",
      "epoch: 0 batch 7300 loss: 0.06893410533666611 correct%: 6.2\n",
      "epoch: 0 batch 7400 loss: 0.06234797090291977 correct%: 7.2\n",
      "epoch: 0 batch 7500 loss: 0.061017755419015884 correct%: 13.0\n",
      "epoch: 0 batch 7600 loss: 0.06247441843152046 correct%: 12.0\n",
      "epoch: 0 batch 7700 loss: 0.062429845333099365 correct%: 15.0\n",
      "epoch: 0 batch 7800 loss: 0.058253444731235504 correct%: 13.8\n",
      "epoch: 0 batch 7900 loss: 0.06522832065820694 correct%: 13.8\n",
      "epoch: 0 loss: 0.06202905997633934\n",
      "X: rsayonrcpzpvthc Pred:     rsayonrcpzp Actual     rsayonrcpzp\n",
      "tensor([ 0,  0,  0,  0, 18, 19,  1, 25, 15, 14, 18,  3, 16, 26, 16],\n",
      "       device='cuda:0') tensor([ 0,  0,  0,  0, 18, 19,  1, 25, 15, 14, 18,  3, 16, 26, 16],\n",
      "       device='cuda:0') tensor(True, device='cuda:0')\n",
      "epoch: 1 batch 0 loss: 0.05974355340003967 correct%: 0.0\n",
      "epoch: 1 batch 100 loss: 0.053019747138023376 correct%: 21.4\n",
      "epoch: 1 batch 200 loss: 0.058899056166410446 correct%: 25.2\n",
      "epoch: 1 batch 300 loss: 0.0589035227894783 correct%: 21.2\n",
      "epoch: 1 batch 400 loss: 0.04959466680884361 correct%: 29.6\n",
      "epoch: 1 batch 500 loss: 0.056397564709186554 correct%: 31.0\n",
      "epoch: 1 batch 600 loss: 0.05441132187843323 correct%: 30.6\n",
      "epoch: 1 batch 700 loss: 0.052469752728939056 correct%: 29.0\n",
      "epoch: 1 batch 800 loss: 0.05911712348461151 correct%: 38.0\n",
      "epoch: 1 batch 900 loss: 0.048197053372859955 correct%: 38.8\n",
      "epoch: 1 batch 1000 loss: 0.05634298920631409 correct%: 39.4\n",
      "epoch: 1 batch 1100 loss: 0.05695014074444771 correct%: 40.0\n",
      "epoch: 1 batch 1200 loss: 0.051527854055166245 correct%: 41.8\n",
      "epoch: 1 batch 1300 loss: 0.05016551911830902 correct%: 40.4\n",
      "epoch: 1 batch 1400 loss: 0.05157501623034477 correct%: 47.6\n",
      "epoch: 1 batch 1500 loss: 0.05561450868844986 correct%: 44.8\n",
      "epoch: 1 batch 1600 loss: 0.05229124799370766 correct%: 45.4\n",
      "epoch: 1 batch 1700 loss: 0.04590253531932831 correct%: 47.6\n",
      "epoch: 1 batch 1800 loss: 0.05019587650895119 correct%: 50.6\n",
      "epoch: 1 batch 1900 loss: 0.05001109465956688 correct%: 52.4\n",
      "epoch: 1 batch 2000 loss: 0.04716359078884125 correct%: 48.6\n",
      "epoch: 1 batch 2100 loss: 0.05074905976653099 correct%: 53.0\n",
      "epoch: 1 batch 2200 loss: 0.05166526511311531 correct%: 54.4\n",
      "epoch: 1 batch 2300 loss: 0.053739842027425766 correct%: 61.0\n",
      "epoch: 1 batch 2400 loss: 0.04812884330749512 correct%: 56.8\n",
      "epoch: 1 batch 2500 loss: 0.046641651540994644 correct%: 60.6\n",
      "epoch: 1 batch 2600 loss: 0.050656821578741074 correct%: 59.6\n",
      "epoch: 1 batch 2700 loss: 0.05388450250029564 correct%: 66.4\n",
      "epoch: 1 batch 2800 loss: 0.04904893785715103 correct%: 63.8\n",
      "epoch: 1 batch 2900 loss: 0.04902734234929085 correct%: 61.2\n",
      "epoch: 1 batch 3000 loss: 0.05077161267399788 correct%: 65.4\n",
      "epoch: 1 batch 3100 loss: 0.05339241772890091 correct%: 67.2\n",
      "epoch: 1 batch 3200 loss: 0.048580653965473175 correct%: 68.6\n",
      "epoch: 1 batch 3300 loss: 0.04906221479177475 correct%: 64.2\n",
      "epoch: 1 batch 3400 loss: 0.04870768263936043 correct%: 75.0\n",
      "epoch: 1 batch 3500 loss: 0.04718727618455887 correct%: 67.8\n",
      "epoch: 1 batch 3600 loss: 0.047157760709524155 correct%: 74.6\n",
      "epoch: 1 batch 3700 loss: 0.04780730977654457 correct%: 73.0\n",
      "epoch: 1 batch 3800 loss: 0.05106699839234352 correct%: 73.8\n",
      "epoch: 1 batch 3900 loss: 0.051823582500219345 correct%: 72.2\n",
      "epoch: 1 batch 4000 loss: 0.04835520684719086 correct%: 74.8\n",
      "epoch: 1 batch 4100 loss: 0.0449531264603138 correct%: 74.6\n",
      "epoch: 1 batch 4200 loss: 0.04981032758951187 correct%: 75.6\n",
      "epoch: 1 batch 4300 loss: 0.05017602816224098 correct%: 77.4\n",
      "epoch: 1 batch 4400 loss: 0.047499194741249084 correct%: 79.4\n",
      "epoch: 1 batch 4500 loss: 0.044837240129709244 correct%: 71.8\n",
      "epoch: 1 batch 4600 loss: 0.04499232396483421 correct%: 80.4\n",
      "epoch: 1 batch 4700 loss: 0.047948192805051804 correct%: 79.0\n",
      "epoch: 1 batch 4800 loss: 0.04863276332616806 correct%: 80.8\n",
      "epoch: 1 batch 4900 loss: 0.047692108899354935 correct%: 83.4\n",
      "epoch: 1 batch 5000 loss: 0.04518745094537735 correct%: 75.4\n",
      "epoch: 1 batch 5100 loss: 0.04504740983247757 correct%: 78.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch 5200 loss: 0.04702805355191231 correct%: 81.4\n",
      "epoch: 1 batch 5300 loss: 0.046983685344457626 correct%: 81.6\n",
      "epoch: 1 batch 5400 loss: 0.053814589977264404 correct%: 84.0\n",
      "epoch: 1 batch 5500 loss: 0.048401106148958206 correct%: 78.8\n",
      "epoch: 1 batch 5600 loss: 0.04688302427530289 correct%: 85.2\n",
      "epoch: 1 batch 5700 loss: 0.04978634789586067 correct%: 82.0\n",
      "epoch: 1 batch 5800 loss: 0.04922318831086159 correct%: 85.2\n",
      "epoch: 1 batch 5900 loss: 0.04472767934203148 correct%: 83.0\n",
      "epoch: 1 batch 6000 loss: 0.048076655715703964 correct%: 86.0\n",
      "epoch: 1 batch 6100 loss: 0.04874015226960182 correct%: 86.6\n",
      "epoch: 1 batch 6200 loss: 0.0457429401576519 correct%: 85.0\n",
      "epoch: 1 batch 6300 loss: 0.048776231706142426 correct%: 84.6\n",
      "epoch: 1 batch 6400 loss: 0.04978688806295395 correct%: 87.6\n",
      "epoch: 1 batch 6500 loss: 0.044879473745822906 correct%: 85.0\n",
      "epoch: 1 batch 6600 loss: 0.04518953710794449 correct%: 86.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6479/1586259276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mtrain_model3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecho_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6479/1586259276.py\u001b[0m in \u001b[0;36mtrain_model3\u001b[0;34m(model, train_dataloader, loss_fn, optimizer, num_epochs, hidden)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m#for batch_idx in range(len(train_data)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, rnn_hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = torch.nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=1,\n",
    "            nonlinearity='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.rnn = torch.nn.GRU(input_size=input_size, hidden_size=rnn_hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(\n",
    "            in_features=rnn_hidden_size,\n",
    "            out_features=output_size\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features=output_size,\n",
    "            out_features=output_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x_orig = x\n",
    "        x, hidden = self.rnn(x, hidden)  \n",
    "        x = self.linear(x)\n",
    "        #x = self.fc(x)\n",
    "        return x, hidden\n",
    "    \n",
    "def train_model3(model, train_dataloader, loss_fn, optimizer, num_epochs, hidden):\n",
    "    model.train()\n",
    "    for train_idx in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        num_samples = 0\n",
    "        correct = 0\n",
    "        for batch_idx, (BX, BY) in enumerate(train_dataloader):\n",
    "        #for batch_idx in range(len(train_data)):\n",
    "            if False:\n",
    "                data, target = train_data[batch_idx]\n",
    "                data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).float().to(device)\n",
    "                BX = data\n",
    "                BY = target\n",
    "            else:\n",
    "                #BX = BX.reshape(BX.shape[0], BX.shape[1], 1)\n",
    "                #BY = BY.reshape(BY.shape[0], BY.shape[1], 1)\n",
    "                BX = BX.to(device)\n",
    "                BY = BY.to(device)\n",
    "                #print(BX.shape)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if hidden is not None: hidden.detach_()\n",
    "            logits, hidden = model(BX, hidden)\n",
    "\n",
    "            loss = loss_fn(logits, BY)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pred = torch.argmax(torch.sigmoid(logits), dim=2)\n",
    "            # THIS IS THE KEY, it can't learn this for some reason\n",
    "            pred[:,0:DELAY] = 0\n",
    "            target2 = torch.argmax(BY.int(), dim=2)\n",
    "            #print(pred[0], target2[0], torch.all(pred.int() == target2, dim=1)[0])\n",
    "            correct += torch.all(pred.int() == target2, dim=1).sum().item()\n",
    "            num_samples += BX.shape[0]\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\"epoch:\", train_idx, \n",
    "                      \"batch\", batch_idx, \n",
    "                      \"loss:\", loss.item(),\n",
    "                      \"correct%:\", float(correct) * 100/num_samples\n",
    "                     )\n",
    "                correct = 0\n",
    "                num_samples = 0\n",
    "        print(\"epoch:\", train_idx, \"loss:\", loss.item())\n",
    "        #print(pred[0:2].int(), pred[0:2].int().shape)\n",
    "        print(\"X:\", onehot_to_str(BX[0:2]), \"Pred:\", onehotints_to_str(pred[0:2]), \"Actual\", onehot_to_str(BY[:2]))\n",
    "        print(pred[0], target2[0], torch.all(pred.int() == target2, dim=1)[0])\n",
    "    \n",
    "\n",
    "model = SimpleRNN(\n",
    "    input_size=NUM_CLASSES,\n",
    "    rnn_hidden_size=NUM_CLASSES * DELAY,\n",
    "    output_size=NUM_CLASSES\n",
    ").to(device)\n",
    "hidden = None\n",
    "print(NUM_CLASSES)\n",
    "        \n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model3(model, echo_dataloader, criterion, optimizer, num_epochs=2, hidden=hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_int_to_str(char_int):\n",
    "    if char_int == 0:\n",
    "        return \" \"\n",
    "    else:\n",
    "        char = chr(char_int + 97 - 1)\n",
    "        return char\n",
    "    \n",
    "def str_to_onehot(s):\n",
    "    mat = torch.zeros((2, len(s), N+1))\n",
    "    \n",
    "    for i in range(len(s)):\n",
    "        char = s[i]\n",
    "        char_int = ord(char.lower())-97 + 1\n",
    "        if char == \" \":\n",
    "            char_int = 0\n",
    "        mat[0,i,char_int] = 1\n",
    "        mat[1,i,char_int] = 1\n",
    "    return mat\n",
    "\n",
    "def onehot_to_str(logits):\n",
    "    pred_char_ints = logits.argmax(dim=2).cpu()\n",
    "    return onehotints_to_str(pred_char_ints)\n",
    "    \n",
    "def onehotints_to_str(pred_char_ints):\n",
    "    pred_str = \"\"\n",
    "    for i in range(pred_char_ints.shape[1]):\n",
    "        char_int = pred_char_ints[0,i].item()\n",
    "        pred_str += char_int_to_str(char_int)\n",
    "    return pred_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNrZqYURcKSl"
   },
   "source": [
    "## Model\n",
    "\n",
    "Now, we want to implement the model. For our purposes, we want to use GRU. The architecture consists of GRU and a decoder. Decoder is responsible for decoding the GRU hidden state to yield a predicting for the next output. The parts you are responsible for filling with your code are marked with `TODO`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mkEEMyvzIMRx"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATASET_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6479/785326349.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#DATASET_SIZE = 20000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#BATCH_SIZE = 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEchoDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASET_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mecho_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATASET_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(1);\n",
    "\n",
    "# Max value of the generated integer. 26 is chosen becuase it's\n",
    "# the number of letters in English alphabet.\n",
    "\n",
    "\n",
    "def turn_into_onehot(x, num_classes):\n",
    "    x_new = torch.zeros((x.shape[0], num_classes,))\n",
    "    for i in range(x.shape[0]):\n",
    "        x_new[i, x[i]] = 1\n",
    "    return x_new\n",
    "\n",
    "class EchoDataset(torch.utils.data.IterableDataset):\n",
    "\n",
    "  def __init__(self, delay=4, seq_length=15, size=1000, num_classes=2):\n",
    "    self.delay = delay\n",
    "    self.seq_length = seq_length\n",
    "    self.size = size\n",
    "    self.num_classes = num_classes\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.size\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\" Iterable dataset doesn't have to implement __getitem__.\n",
    "        Instead, we only need to implement __iter__ to return\n",
    "        an iterator (or generator).\n",
    "    \"\"\"\n",
    "    for _ in range(self.size):\n",
    "      seq = torch.tensor([random.choice(range(0, self.num_classes)) for i in range(self.seq_length)], dtype=torch.int64)\n",
    "      #result = torch.cat((torch.zeros(self.delay), seq[:self.seq_length - self.delay])).type(torch.int64)\n",
    "      seq = turn_into_onehot(seq, self.num_classes)\n",
    "      result = torch.roll(seq, self.delay, dims=0)\n",
    "      result[:self.delay,:] = 0\n",
    "      result[:self.delay,0] = 1\n",
    "      #result = turn_into_onehot(result, self.num_classes)\n",
    "\n",
    "      yield seq, result\n",
    "\n",
    "#DELAY = 3\n",
    "#NUM_CLASSES = 27\n",
    "#DATASET_SIZE = 20000\n",
    "#BATCH_SIZE = 5\n",
    "ds = EchoDataset(delay=DELAY, size=DATASET_SIZE, num_classes=NUM_CLASSES)\n",
    "echo_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "DX, DY = next(iter(echo_dataloader))\n",
    "print(DX[0], DY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "nigN_o4Mb9Nx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 batch 0 loss: 0.7000271081924438 correct%: 0.0\n",
      "epoch: 0 batch 100 loss: 0.1381944864988327 correct%: 0.0\n",
      "epoch: 0 batch 200 loss: 0.12589429318904877 correct%: 0.0\n",
      "epoch: 0 batch 300 loss: 0.12078474462032318 correct%: 0.0\n",
      "epoch: 0 batch 400 loss: 0.11878122389316559 correct%: 0.0\n",
      "epoch: 0 batch 500 loss: 0.11707454919815063 correct%: 0.0\n",
      "epoch: 0 batch 600 loss: 0.11724920570850372 correct%: 0.0\n",
      "epoch: 0 batch 700 loss: 0.11528179794549942 correct%: 0.0\n",
      "epoch: 0 batch 800 loss: 0.11901368945837021 correct%: 0.0\n",
      "epoch: 0 batch 900 loss: 0.11679990589618683 correct%: 0.0\n",
      "epoch: 0 batch 1000 loss: 0.11554388701915741 correct%: 0.0\n",
      "epoch: 0 batch 1100 loss: 0.11665476858615875 correct%: 0.0\n",
      "epoch: 0 batch 1200 loss: 0.11602777987718582 correct%: 0.0\n",
      "epoch: 0 batch 1300 loss: 0.11546003818511963 correct%: 0.0\n",
      "epoch: 0 batch 1400 loss: 0.11599144339561462 correct%: 0.0\n",
      "epoch: 0 batch 1500 loss: 0.1162181869149208 correct%: 0.0\n",
      "epoch: 0 batch 1600 loss: 0.11555540561676025 correct%: 0.0\n",
      "epoch: 0 batch 1700 loss: 0.1141754761338234 correct%: 0.0\n",
      "epoch: 0 batch 1800 loss: 0.11334005743265152 correct%: 0.0\n",
      "epoch: 0 batch 1900 loss: 0.11440829187631607 correct%: 0.0\n",
      "epoch: 0 batch 2000 loss: 0.11114831268787384 correct%: 0.0\n",
      "epoch: 0 batch 2100 loss: 0.10841410607099533 correct%: 0.0\n",
      "epoch: 0 batch 2200 loss: 0.10524222999811172 correct%: 0.0\n",
      "epoch: 0 batch 2300 loss: 0.10223183780908585 correct%: 0.0\n",
      "epoch: 0 batch 2400 loss: 0.10262734442949295 correct%: 0.0\n",
      "epoch: 0 batch 2500 loss: 0.10841959714889526 correct%: 0.0\n",
      "epoch: 0 batch 2600 loss: 0.09543538093566895 correct%: 0.0\n",
      "epoch: 0 batch 2700 loss: 0.09591663628816605 correct%: 0.0\n",
      "epoch: 0 batch 2800 loss: 0.09225388616323471 correct%: 0.0\n",
      "epoch: 0 batch 2900 loss: 0.08943860977888107 correct%: 0.0\n",
      "epoch: 0 batch 3000 loss: 0.08935528993606567 correct%: 0.0\n",
      "epoch: 0 batch 3100 loss: 0.07748200744390488 correct%: 0.0\n",
      "epoch: 0 batch 3200 loss: 0.07109536230564117 correct%: 0.0\n",
      "epoch: 0 batch 3300 loss: 0.07467151433229446 correct%: 0.0\n",
      "epoch: 0 batch 3400 loss: 0.07105714082717896 correct%: 0.0\n",
      "epoch: 0 batch 3500 loss: 0.0640387088060379 correct%: 0.0\n",
      "epoch: 0 batch 3600 loss: 0.056310009211301804 correct%: 0.4\n",
      "epoch: 0 batch 3700 loss: 0.05593780428171158 correct%: 0.2\n",
      "epoch: 0 batch 3800 loss: 0.05426527187228203 correct%: 1.2\n",
      "epoch: 0 batch 3900 loss: 0.0513409785926342 correct%: 3.4\n",
      "epoch: 0 batch 4000 loss: 0.04612194746732712 correct%: 1.6\n",
      "epoch: 0 batch 4100 loss: 0.04720170795917511 correct%: 6.4\n",
      "epoch: 0 batch 4200 loss: 0.037873949855566025 correct%: 8.0\n",
      "epoch: 0 batch 4300 loss: 0.03431236743927002 correct%: 18.0\n",
      "epoch: 0 batch 4400 loss: 0.032683927565813065 correct%: 17.8\n",
      "epoch: 0 batch 4500 loss: 0.033178672194480896 correct%: 19.2\n",
      "epoch: 0 batch 4600 loss: 0.025842582806944847 correct%: 30.6\n",
      "epoch: 0 batch 4700 loss: 0.022957228124141693 correct%: 33.2\n",
      "epoch: 0 batch 4800 loss: 0.02432580105960369 correct%: 38.0\n",
      "epoch: 0 batch 4900 loss: 0.019217679277062416 correct%: 48.8\n",
      "epoch: 0 batch 5000 loss: 0.023135827854275703 correct%: 46.2\n",
      "epoch: 0 batch 5100 loss: 0.024557659402489662 correct%: 56.6\n",
      "epoch: 0 batch 5200 loss: 0.01677653379738331 correct%: 60.2\n",
      "epoch: 0 batch 5300 loss: 0.0170515775680542 correct%: 67.8\n",
      "epoch: 0 batch 5400 loss: 0.01844753324985504 correct%: 75.4\n",
      "epoch: 0 batch 5500 loss: 0.01687573827803135 correct%: 77.6\n",
      "epoch: 0 batch 5600 loss: 0.014503156766295433 correct%: 79.0\n",
      "epoch: 0 batch 5700 loss: 0.008981242775917053 correct%: 82.6\n",
      "epoch: 0 batch 5800 loss: 0.005700408946722746 correct%: 90.2\n",
      "epoch: 0 batch 5900 loss: 0.007672934792935848 correct%: 91.8\n",
      "epoch: 0 batch 6000 loss: 0.006570624187588692 correct%: 94.8\n",
      "epoch: 0 batch 6100 loss: 0.006822604686021805 correct%: 95.2\n",
      "epoch: 0 batch 6200 loss: 0.00584213575348258 correct%: 96.8\n",
      "epoch: 0 batch 6300 loss: 0.006093472242355347 correct%: 97.0\n",
      "epoch: 0 batch 6400 loss: 0.0024944592732936144 correct%: 96.2\n",
      "epoch: 0 batch 6500 loss: 0.00261495728045702 correct%: 98.0\n",
      "epoch: 0 batch 6600 loss: 0.002198247704654932 correct%: 95.6\n",
      "epoch: 0 batch 6700 loss: 0.0035052329767495394 correct%: 98.2\n",
      "epoch: 0 batch 6800 loss: 0.002855367725715041 correct%: 99.0\n",
      "epoch: 0 batch 6900 loss: 0.0029244397301226854 correct%: 99.4\n",
      "epoch: 0 batch 7000 loss: 0.0013578643556684256 correct%: 100.0\n",
      "epoch: 0 batch 7100 loss: 0.002366397064179182 correct%: 96.2\n",
      "epoch: 0 batch 7200 loss: 0.0009434801177121699 correct%: 98.8\n",
      "epoch: 0 batch 7300 loss: 0.0015560067258775234 correct%: 100.0\n",
      "epoch: 0 batch 7400 loss: 0.001075376057997346 correct%: 98.2\n",
      "epoch: 0 batch 7500 loss: 0.0011958011891692877 correct%: 100.0\n",
      "epoch: 0 batch 7600 loss: 0.0012082539033144712 correct%: 96.4\n",
      "epoch: 0 batch 7700 loss: 0.0005507279420271516 correct%: 100.0\n",
      "epoch: 0 batch 7800 loss: 0.0006358539103530347 correct%: 100.0\n",
      "epoch: 0 batch 7900 loss: 0.0004916117759421468 correct%: 100.0\n",
      "epoch: 0 loss: 0.0005294149741530418\n",
      "X: mszhobjldurvpcl Pred:     mszhobjldur Actual     mszhobjldur\n",
      "tensor([ 0,  0,  0,  0, 13, 19, 26,  8, 15,  2, 10, 12,  4, 21, 18],\n",
      "       device='cuda:0') tensor([ 0,  0,  0,  0, 13, 19, 26,  8, 15,  2, 10, 12,  4, 21, 18],\n",
      "       device='cuda:0') tensor(True, device='cuda:0')\n",
      "epoch: 1 batch 0 loss: 0.0005943927681073546 correct%: 100.0\n",
      "epoch: 1 batch 100 loss: 0.00040436044218949974 correct%: 100.0\n",
      "epoch: 1 batch 200 loss: 0.0007748041534796357 correct%: 100.0\n",
      "epoch: 1 batch 300 loss: 0.0004489992861635983 correct%: 100.0\n",
      "epoch: 1 batch 400 loss: 0.0005204420886002481 correct%: 97.4\n",
      "epoch: 1 batch 500 loss: 0.0003171195858158171 correct%: 100.0\n",
      "epoch: 1 batch 600 loss: 0.001302681164816022 correct%: 97.2\n",
      "epoch: 1 batch 700 loss: 0.00021936926350463182 correct%: 100.0\n",
      "epoch: 1 batch 800 loss: 0.00028090045088902116 correct%: 100.0\n",
      "epoch: 1 batch 900 loss: 0.00016624182171653956 correct%: 100.0\n",
      "epoch: 1 batch 1000 loss: 0.0002135515824193135 correct%: 100.0\n",
      "epoch: 1 batch 1100 loss: 0.0004633341159205884 correct%: 97.0\n",
      "epoch: 1 batch 1200 loss: 0.00015892951341811568 correct%: 100.0\n",
      "epoch: 1 batch 1300 loss: 0.00014911001198925078 correct%: 100.0\n",
      "epoch: 1 batch 1400 loss: 0.00013306929031386971 correct%: 100.0\n",
      "epoch: 1 batch 1500 loss: 0.00010900502820732072 correct%: 100.0\n",
      "epoch: 1 batch 1600 loss: 0.006136953830718994 correct%: 96.4\n",
      "epoch: 1 batch 1700 loss: 0.00023584914742968976 correct%: 99.2\n",
      "epoch: 1 batch 1800 loss: 0.00010474921145942062 correct%: 100.0\n",
      "epoch: 1 batch 1900 loss: 0.0001253137888852507 correct%: 100.0\n",
      "epoch: 1 batch 2000 loss: 0.00017714034765958786 correct%: 97.0\n",
      "epoch: 1 batch 2100 loss: 0.00010410961112938821 correct%: 99.2\n",
      "epoch: 1 batch 2200 loss: 0.00011476026702439412 correct%: 100.0\n",
      "epoch: 1 batch 2300 loss: 8.202129538403824e-05 correct%: 100.0\n",
      "epoch: 1 batch 2400 loss: 9.01304665603675e-05 correct%: 100.0\n",
      "epoch: 1 batch 2500 loss: 7.38063536118716e-05 correct%: 100.0\n",
      "epoch: 1 batch 2600 loss: 0.0001091387530323118 correct%: 98.0\n",
      "epoch: 1 batch 2700 loss: 0.00010378131264587864 correct%: 100.0\n",
      "epoch: 1 batch 2800 loss: 7.708243356319144e-05 correct%: 100.0\n",
      "epoch: 1 batch 2900 loss: 6.934353586984798e-05 correct%: 100.0\n",
      "epoch: 1 batch 3000 loss: 6.101177859818563e-05 correct%: 100.0\n",
      "epoch: 1 batch 3100 loss: 5.0734721298795193e-05 correct%: 100.0\n",
      "epoch: 1 batch 3200 loss: 0.0006727467407472432 correct%: 97.6\n",
      "epoch: 1 batch 3300 loss: 7.314937829505652e-05 correct%: 100.0\n",
      "epoch: 1 batch 3400 loss: 6.357191887218505e-05 correct%: 100.0\n",
      "epoch: 1 batch 3500 loss: 6.570396362803876e-05 correct%: 100.0\n",
      "epoch: 1 batch 3600 loss: 4.468876795726828e-05 correct%: 100.0\n",
      "epoch: 1 batch 3700 loss: 4.5547945774160326e-05 correct%: 100.0\n",
      "epoch: 1 batch 3800 loss: 7.890211418271065e-05 correct%: 100.0\n",
      "epoch: 1 batch 3900 loss: 3.5373068385524675e-05 correct%: 100.0\n",
      "epoch: 1 batch 4000 loss: 8.319431799463928e-05 correct%: 97.4\n",
      "epoch: 1 batch 4100 loss: 5.794865501229651e-05 correct%: 100.0\n",
      "epoch: 1 batch 4200 loss: 4.08483792853076e-05 correct%: 100.0\n",
      "epoch: 1 batch 4300 loss: 0.00013804802438244224 correct%: 99.8\n",
      "epoch: 1 batch 4400 loss: 4.373403135105036e-05 correct%: 97.2\n",
      "epoch: 1 batch 4500 loss: 4.999695738661103e-05 correct%: 100.0\n",
      "epoch: 1 batch 4600 loss: 3.92247129639145e-05 correct%: 100.0\n",
      "epoch: 1 batch 4700 loss: 3.152258796035312e-05 correct%: 100.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch 4800 loss: 5.582403173320927e-05 correct%: 100.0\n",
      "epoch: 1 batch 4900 loss: 2.6146606614929624e-05 correct%: 100.0\n",
      "epoch: 1 batch 5000 loss: 2.672799200809095e-05 correct%: 100.0\n",
      "epoch: 1 batch 5100 loss: 2.4974613552330993e-05 correct%: 100.0\n",
      "epoch: 1 batch 5200 loss: 5.069262260803953e-05 correct%: 97.0\n",
      "epoch: 1 batch 5300 loss: 4.307605922804214e-05 correct%: 100.0\n",
      "epoch: 1 batch 5400 loss: 2.8165377443656325e-05 correct%: 100.0\n",
      "epoch: 1 batch 5500 loss: 2.574163045210298e-05 correct%: 100.0\n",
      "epoch: 1 batch 5600 loss: 2.4531304006814025e-05 correct%: 100.0\n",
      "epoch: 1 batch 5700 loss: 0.00016950452118180692 correct%: 96.0\n",
      "epoch: 1 batch 5800 loss: 0.00019201914255972952 correct%: 99.6\n",
      "epoch: 1 batch 5900 loss: 3.1066476367414e-05 correct%: 100.0\n",
      "epoch: 1 batch 6000 loss: 3.433397796470672e-05 correct%: 100.0\n",
      "epoch: 1 batch 6100 loss: 3.808172550634481e-05 correct%: 100.0\n",
      "epoch: 1 batch 6200 loss: 2.245393261546269e-05 correct%: 100.0\n",
      "epoch: 1 batch 6300 loss: 1.9182254618499428e-05 correct%: 100.0\n",
      "epoch: 1 batch 6400 loss: 2.27111504500499e-05 correct%: 100.0\n",
      "epoch: 1 batch 6500 loss: 4.791588071384467e-05 correct%: 98.2\n",
      "epoch: 1 batch 6600 loss: 1.8397522580926307e-05 correct%: 100.0\n",
      "epoch: 1 batch 6700 loss: 3.0932071240385994e-05 correct%: 100.0\n",
      "epoch: 1 batch 6800 loss: 0.00012503161269705743 correct%: 98.0\n",
      "epoch: 1 batch 6900 loss: 2.7158024749951437e-05 correct%: 100.0\n",
      "epoch: 1 batch 7000 loss: 3.018863390025217e-05 correct%: 100.0\n",
      "epoch: 1 batch 7100 loss: 2.925057378888596e-05 correct%: 100.0\n",
      "epoch: 1 batch 7200 loss: 2.5365232431795448e-05 correct%: 100.0\n",
      "epoch: 1 batch 7300 loss: 1.9224942661821842e-05 correct%: 100.0\n",
      "epoch: 1 batch 7400 loss: 1.3717030014959164e-05 correct%: 100.0\n",
      "epoch: 1 batch 7500 loss: 3.240664227632806e-05 correct%: 97.8\n",
      "epoch: 1 batch 7600 loss: 4.785329656442627e-05 correct%: 100.0\n",
      "epoch: 1 batch 7700 loss: 3.534097777446732e-05 correct%: 100.0\n",
      "epoch: 1 batch 7800 loss: 1.9075590898864903e-05 correct%: 100.0\n",
      "epoch: 1 batch 7900 loss: 3.254730108892545e-05 correct%: 100.0\n",
      "epoch: 1 loss: 9.807734386413358e-06\n",
      "X: swlgafyfbcvmnue Pred:     swlgafyfbcv Actual     swlgafyfbcv\n",
      "tensor([ 0,  0,  0,  0, 19, 23, 12,  7,  1,  6, 25,  6,  2,  3, 22],\n",
      "       device='cuda:0') tensor([ 0,  0,  0,  0, 19, 23, 12,  7,  1,  6, 25,  6,  2,  3, 22],\n",
      "       device='cuda:0') tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "N = NUM_CLASSES\n",
    "\n",
    "class GRUMemory(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size=NUM_CLASSES):\n",
    "    super().__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    \n",
    "    self.input_to_reset = torch.nn.Linear(input_size, hidden_size)\n",
    "    self.hidden_to_reset = torch.nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    self.input_to_update = torch.nn.Linear(input_size, hidden_size)\n",
    "    self.hidden_to_update = torch.nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    self.input_to_new = torch.nn.Linear(input_size, hidden_size)\n",
    "    self.hidden_to_new = torch.nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    self.x2h = nn.Linear(input_size, 3 * hidden_size)\n",
    "    self.h2h = nn.Linear(hidden_size, 3 * hidden_size)\n",
    "    \n",
    "  def forward_imp(self, x, hidden):\n",
    "    \n",
    "    ir = self.input_to_reset(x)\n",
    "    hr = self.hidden_to_reset(hidden)\n",
    "    reset_gate_out = F.sigmoid(ir + hr) # R\n",
    "    \n",
    "    iu = self.input_to_update(x)\n",
    "    hu = self.hidden_to_update(hidden)\n",
    "    update_gate_out = F.sigmoid(iu + hu) # Z\n",
    "    \n",
    "    i_n = self.input_to_new(x)\n",
    "\n",
    "    new_gate = F.tanh(i_n + self.hidden_to_new(reset_gate_out * hidden)) #H_tilda\n",
    "    hidden_new = (1.0 - update_gate_out) * new_gate + update_gate_out * hidden #H\n",
    "    return hidden_new\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    # inputs: x - input tensor of shape (batch_size, seq_length, N+1)\n",
    "    # returns:\n",
    "    # logits (scores for softmax) of shape (batch size, seq_length, N + 1)\n",
    "    # TODO implement forward pass\n",
    "    outputs = torch.zeros((x.shape[0], x.shape[1], self.output_size)).to(device)\n",
    "    \n",
    "    hn = hidden[:,0,:]\n",
    "    for seq in range(x.shape[1]):\n",
    "        #hn = hidden[:,seq,:]\n",
    "        hn = self.forward_imp(x[:,seq,:], hn)\n",
    "        outputs[:, seq, :] = self.linear(hn)\n",
    "    \n",
    "    #output2 = self.linear(output)\n",
    "    return outputs, hidden\n",
    "\n",
    "  def init_hidden(self, x):\n",
    "    hidden = torch.autograd.Variable(torch.zeros(x.shape[0], x.shape[1]+1, self.hidden_size)).to(device)\n",
    "    return hidden\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def test_run(self, s):\n",
    "    # This function accepts one string s containing lowercase characters a-z. \n",
    "    # You need to map those characters to one-hot encodings, \n",
    "    # then get the result from your network, and then convert the output \n",
    "    # back to a string of the same length, with 0 mapped to ' ', \n",
    "    # and 1-26 mapped to a-z.\n",
    "    mat = torch.zeros((2, len(s), self.input_size))\n",
    "    \n",
    "    for i in range(len(s)):\n",
    "        char = s[i]\n",
    "        char_int = ord(char.lower())-97 + 1\n",
    "        if char == \" \":\n",
    "            char_int = 0\n",
    "        mat[0,i,char_int] = 1\n",
    "        mat[1,i,char_int] = 1\n",
    "        \n",
    "    mat = mat.to(device)\n",
    "    hidden = self.init_hidden(mat)\n",
    "    logits, hidden = self.forward(mat, hidden)\n",
    "    pred_char_ints = logits.argmax(dim=2).cpu()\n",
    "    \n",
    "    pred_str = \"\"\n",
    "    pred_str2 = \"\"\n",
    "    \n",
    "    for i in range(pred_char_ints.shape[1]):\n",
    "        char_int = pred_char_ints[0,i].item()\n",
    "        char_int2 = pred_char_ints[1,i].item()\n",
    "        pred_str += char_int_to_str(char_int)\n",
    "        pred_str2 += char_int_to_str(char_int2)\n",
    "    print(\"Pred:\", pred_str, \"2\", pred_str2)\n",
    "    return pred_str\n",
    "\n",
    "def char_int_to_str(char_int):\n",
    "    if char_int == 0:\n",
    "        return \" \"\n",
    "    else:\n",
    "        char = chr(char_int + 97 - 1)\n",
    "        return char\n",
    "    \n",
    "def str_to_onehot(s):\n",
    "    mat = torch.zeros((2, len(s), N+1))\n",
    "    \n",
    "    for i in range(len(s)):\n",
    "        char = s[i]\n",
    "        char_int = ord(char.lower())-97 + 1\n",
    "        if char == \" \":\n",
    "            char_int = 0\n",
    "        mat[0,i,char_int] = 1\n",
    "        mat[1,i,char_int] = 1\n",
    "    return mat\n",
    "\n",
    "def onehot_to_str(logits):\n",
    "    pred_char_ints = logits.argmax(dim=2).cpu()\n",
    "    return onehotints_to_str(pred_char_ints)\n",
    "    \n",
    "def onehotints_to_str(pred_char_ints):\n",
    "    pred_str = \"\"\n",
    "    for i in range(pred_char_ints.shape[1]):\n",
    "        char_int = pred_char_ints[0,i].item()\n",
    "        pred_str += char_int_to_str(char_int)\n",
    "    return pred_str\n",
    "    \n",
    "model_gru = GRUMemory(input_size=NUM_CLASSES, hidden_size=DELAY*NUM_CLASSES).to(device)\n",
    "\n",
    "#model_gru.test_run(\"hello there\")\n",
    "\n",
    "criterion_gru = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer_gru = torch.optim.RMSprop(model_gru.parameters(), lr=0.001)\n",
    "\n",
    "hidden=model_gru.init_hidden(next(iter(echo_dataloader))[0])\n",
    "train_model3(model_gru, echo_dataloader, criterion_gru, optimizer_gru, num_epochs=2, hidden=hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9whwmVu9OIx"
   },
   "source": [
    "## Training\n",
    "Below you need to implement the training of the model. We give you more freedom as for the implementation. The two limitations are that it has to execute within 10 minutes, and that error rate should be below 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUZkeRnVTNzG"
   },
   "outputs": [],
   "source": [
    "D = DELAY\n",
    "def test_model(model, sequence_length=15):\n",
    "  \"\"\"\n",
    "  This is the test function that runs 100 different strings through your model,\n",
    "  and checks the error rate.\n",
    "  \"\"\"\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  for i in range(500):\n",
    "    s = ''.join([random.choice(string.ascii_lowercase) for i in range(random.randint(15, 25))])\n",
    "    result = model.test_run(s)\n",
    "    assert D > 0, 's[:-D] won\\'t work for D=0'\n",
    "    print(\"res:\", result, s[:-D], result[D:])\n",
    "    for c1, c2 in zip(s[:-D], result[D:]):\n",
    "      correct += int(c1 == c2)\n",
    "    total += len(s) - D\n",
    "\n",
    "  return correct / total\n",
    "\n",
    "def train_model(model, train_dataloader, loss_fn, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for train_idx in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        num_samples = 0\n",
    "        for batch_idx, (BX, BY) in enumerate(train_dataloader):\n",
    "            #BX = BX.reshape(BX.shape[0], BX.shape[1], 1)\n",
    "            #BY = BY.reshape(BY.shape[0], BY.shape[1], 1)\n",
    "            BX = BX.to(device)\n",
    "            BY = BY.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            Yhat = model(BX)\n",
    "            loss = loss_fn(Yhat, BY)\n",
    "            total_loss += loss.item()\n",
    "            num_samples+= BX.shape[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\"epoch:\", train_idx, \"batch\", batch_idx, \"loss:\", total_loss/num_samples)\n",
    "        print(\"epoch:\", train_idx, \"loss:\", total_loss/num_samples)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lV9BscxCCAI"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# TODO: initialize and train your model here.\n",
    "model = GRUMemory(input_size=N+1, hidden_size=6).to(device)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "text = \"hello there\"\n",
    "print(model.test_run(text))\n",
    "train_model(model, echo_dataloader, loss_fn, optimizer, num_epochs=1)\n",
    "\n",
    "def do_test_model():\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    accuracy = test_model(model)\n",
    "    assert duration < 600, 'execution took f{duration:.2f} seconds, which longer than 10 mins'\n",
    "    assert accuracy > 0.99, f'accuracy is too low, got {accuracy}, need 0.99'\n",
    "    print('tests passed')\n",
    "    \n",
    "    \n",
    "print(\"After:\", model.test_run(text))\n",
    "#do_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "\n",
    "class RNNWrapper(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size=1):\n",
    "    super().__init__()\n",
    "    self.rnn = torch.nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "    self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "    self.linear2 = torch.nn.Linear(output_size, output_size)\n",
    "    self.hidden=None\n",
    "    \n",
    "  def forward(self, x, h0):\n",
    "    #num_hidden = 4\n",
    "    #out = torch.roll(x, shifts=num_hidden, dims=1)\n",
    "    #out[:,:num_hidden,:] = 0\n",
    "    #out[:,:num_hidden,0] = 1\n",
    "    #return out\n",
    "    output, hidden = self.rnn(x, h0)\n",
    "    return output\n",
    "    #print(\"Output shape:\", output.shape, \"Hidden shape:\", hidden.shape)\n",
    "    return self.linear(torch.sigmoid(output))\n",
    "\n",
    "\n",
    "hidden_size = 4\n",
    "rnn = RNNWrapper(input_size=1, hidden_size=hidden_size).to(device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.RMSprop(rnn.parameters(), lr=30)\n",
    "#input = torch.randn(3, 5, 10)\n",
    "#h0 = torch.randn(1, 3, 20)\n",
    "#output, hn = rnn(input, h0)\n",
    "#print(output.shape, hn.shape)\n",
    "#print([p.shape for p in rnn.parameters()])\n",
    "\n",
    "def train_model2(model, train_dataloader, loss_fn, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for train_idx in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        num_samples = 0\n",
    "        for batch_idx, (BX, BY) in enumerate(train_dataloader):\n",
    "            #BX = BX.reshape(BX.shape[0], BX.shape[1], 1)\n",
    "            #BY = BY.reshape(BY.shape[0], BY.shape[1], 1)\n",
    "            BX = BX.to(device)\n",
    "            BY = BY.to(device)\n",
    "            print(BX.shape)\n",
    "            hidden = torch.zeros(num_layers, BX.shape[0], hidden_size).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            Yhat = model(BX, hidden)\n",
    "            #print(\"Input:\", onehot_to_str(BX[0:1]), \"Expected:\", onehot_to_str(BY[0:1]), \"Guess:\", onehot_to_str(Yhat[0:1]))\n",
    "            #loss = loss_fn(Yhat[0:1], BY[0:1])\n",
    "            #print(\"Loss should be 0\", loss)\n",
    "            #return\n",
    "            loss = loss_fn(Yhat, BY)\n",
    "            total_loss += loss.item()\n",
    "            num_samples+= BX.shape[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\"epoch:\", train_idx, \"batch\", batch_idx, \"loss:\", total_loss/num_samples)\n",
    "        print(\"epoch:\", train_idx, \"loss:\", total_loss/num_samples)\n",
    "        hidden = torch.zeros(num_layers, 2, hidden_size).to(device)\n",
    "        pred = rnn(BX[0:2, :, :], hidden)\n",
    "        #print(\"Pred:\", onehot_to_str(pred), \"Actual\", onehot_to_str(BY[:2]))\n",
    "        \n",
    "hidden = torch.zeros(num_layers, 2, hidden_size).to(device)\n",
    "#pred = rnn(str_to_onehot(\"hello there\").to(device), hidden)\n",
    "#print(\"Before:\", onehot_to_str(pred))\n",
    "train_model2(rnn, echo_dataloader, loss_fn, optimizer, num_epochs=10)\n",
    "#pred = rnn(str_to_onehot(\"hello there\").to(device), hidden)\n",
    "#print(\"After:\", onehot_to_str(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((2, 5))\n",
    "a[1] = 2\n",
    "b = torch.zeros((2, 5))\n",
    "torch.all(a == b, dim=1)\n",
    "random.choice(range(0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB0EVNBtDhpN"
   },
   "source": [
    "## Variable delay model\n",
    "\n",
    "Now, to make this more complicated, we want to have varialbe delay. So, now, the goal is to transform a sequence of pairs (character, delay) into a character sequence with given delay. Delay stays constant within one sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i_iwX_AEOCH"
   },
   "source": [
    "### Dataset\n",
    "As before, we first implement the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4G5b8kuEUEd"
   },
   "outputs": [],
   "source": [
    "class VariableDelayEchoDataset(torch.utils.data.IterableDataset):\n",
    "\n",
    "  def __init__(self, max_delay=8, seq_length=20, size=1000):\n",
    "    self.max_delay = max_delay\n",
    "    self.seq_length = seq_length\n",
    "    self.size = size\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.size\n",
    "\n",
    "  def __iter__(self):\n",
    "    for _ in range(self.size):\n",
    "      seq = torch.tensor([random.choice(range(1, N + 1)) for i in range(self.seq_length)], dtype=torch.int64)\n",
    "      delay = random.randint(0, self.max_delay)\n",
    "      result = torch.cat((torch.zeros(delay), seq[:self.seq_length - delay])).type(torch.int64)\n",
    "      yield seq, delay, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTRVOND3HEJZ"
   },
   "source": [
    "### Model\n",
    "\n",
    "And the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYolFIB8Hg0U"
   },
   "outputs": [],
   "source": [
    "class VariableDelayGRUMemory(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, hidden_size, max_delay):\n",
    "    super().__init__()\n",
    "    #TODO\n",
    "\n",
    "  def forward(self, x, delays):\n",
    "    # inputs:\n",
    "    # x - tensor of shape (batch size, seq length, N + 1)\n",
    "    # delays - tensor of shape (batch size)\n",
    "    # returns:\n",
    "    # logits (scores for softmax) of shape (batch size, seq_length, N + 1)\n",
    "\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def test_run(self, s, delay):\n",
    "    # This function accepts one string s containing lowercase characters a-z, \n",
    "    # and a delay - the desired output delay.\n",
    "    # You need to map those characters to one-hot encodings, \n",
    "    # then get the result from your network, and then convert the output \n",
    "    # back to a string of the same length, with 0 mapped to ' ', \n",
    "    # and 1-26 mapped to a-z.\n",
    "\n",
    "    # TODO\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riu3qHWgKjsx"
   },
   "source": [
    "### Train\n",
    "\n",
    "As before, you're free to do what you want, as long as training finishes within 10 minutes and accuracy is above 0.99 for delays between 0 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FZHojnGO3aw"
   },
   "outputs": [],
   "source": [
    "def test_variable_delay_model(model, seq_length=20):\n",
    "  \"\"\"\n",
    "  This is the test function that runs 100 different strings through your model,\n",
    "  and checks the error rate.\n",
    "  \"\"\"\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  for i in range(500):\n",
    "    s = ''.join([random.choice(string.ascii_lowercase) for i in range(seq_length)])\n",
    "    d = random.randint(0, model.max_delay)\n",
    "    result = model.test_run(s, d)\n",
    "    if d > 0:\n",
    "      z = zip(s[:-d], result[d:])\n",
    "    else:\n",
    "      z = zip(s, result)\n",
    "    for c1, c2 in z:\n",
    "      correct += int(c1 == c2)\n",
    "    total += len(s) - d\n",
    "\n",
    "  return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJ18Ef6vKi4s"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "MAX_DELAY = 8\n",
    "SEQ_LENGTH = 20\n",
    "\n",
    "# TODO: implement model training here.\n",
    "model = None\n",
    "\n",
    "end_time = time.time()\n",
    "assert end_time - start_time < 600, 'executing took longer than 10 mins'\n",
    "assert test_variable_delay_model(model) > 0.99, 'accuracy is too low'\n",
    "print('tests passed')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JUbvDw31eJvY"
   ],
   "name": "hw2_rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
