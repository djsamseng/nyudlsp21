{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc7a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ipycanvas\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6231e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4160, -0.0209]) tensor([-0.1366, -0.0071]) tensor([-0.4160, -0.0209], grad_fn=<MulBackward0>) tensor([-0.1366, -0.0071], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def compute(x, h1, h2, gt1_w1, gt1_w2, w1, w2):\n",
    "    # Single recursive node\n",
    "    y = w1 * x + w2 * h1\n",
    "    # dh1/dh2 * dh2/dw + dh1/dw\n",
    "    gt_w1 = w2 * gt1_w1 + x\n",
    "    gt_w2 = w2 * gt1_w2 + h1\n",
    "    return y, gt_w1, gt_w2\n",
    "\n",
    "\n",
    "w1 = torch.rand(2, requires_grad=True)\n",
    "w2 = torch.rand(2, requires_grad=True)\n",
    "h0 = 0\n",
    "x1 = torch.rand(2, requires_grad=True)\n",
    "h1, dh_w1, dh_w2 = compute(x1, h0, 0, 0, 0, w1, w2)\n",
    "#assert h1 == 1\n",
    "\n",
    "x2 = torch.rand(2, requires_grad=True)\n",
    "h2, dh_w1, dh_w2 = compute(x2, h1, h0, dh_w1, dh_w2, w1, w2)\n",
    "#assert h2 == (1.5 + 0.5)\n",
    "\n",
    "x3 = torch.rand(2, requires_grad=True)\n",
    "h3, dh_w1, dh_w2 = compute(x3, h2, h1, dh_w1, dh_w2, w1, w2)\n",
    "\n",
    "y = torch.rand(2)\n",
    "loss = torch.nn.MSELoss()(h3, y)\n",
    "loss.backward()\n",
    "\n",
    "error_grad = 2 * (h3 - y) / 2\n",
    "# YAY! gt1_w1 is correct! With gt_w1 = w2 * gt1_w1 + x\n",
    "# YAY! gt1_w2 is correct! With gt_w2 = w2 * gt1_w2 + h1\n",
    "print(w1.grad, w2.grad, dh_w1*error_grad, dh_w2*error_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46bd3a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=200, width=800)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "c = ipycanvas.Canvas(width=800, height=200)\n",
    "\n",
    "def draw_node(c, x, y):\n",
    "    c.stroke_style = \"red\"\n",
    "    c.stroke_circle(x, y, 40)\n",
    "\n",
    "def draw_recurrent_node(c, x=60, y=100):\n",
    "\n",
    "    draw_node(c, x, y)\n",
    "\n",
    "    c.begin_path()\n",
    "    y_arc = y - 30\n",
    "    x0 = x+20\n",
    "    x1 = x-20\n",
    "    c.move_to(x0, y_arc)\n",
    "    c.quadratic_curve_to(x0 + 5, y_arc-50, x0 + (x1 - x0)//2, y_arc - 50)\n",
    "    c.quadratic_curve_to(x1 - 5, y_arc-50, x1, y_arc)\n",
    "    c.stroke()\n",
    "    c.begin_path()\n",
    "    arrow_width = 5\n",
    "    c.line_to(x1-arrow_width, y_arc)\n",
    "    c.line_to(x1, y_arc+arrow_width)\n",
    "    c.line_to(x1+arrow_width, y_arc)\n",
    "    c.fill()\n",
    "    \n",
    "def draw_arrow(c, x0, y0, x1, y1, arrow_width=5, direction=\"right\"):\n",
    "    c.begin_path()\n",
    "    c.move_to(x0, y0)\n",
    "    c.line_to(x1, y1)\n",
    "    c.stroke()\n",
    "    c.begin_path()\n",
    "    if direction == \"right\":\n",
    "        c.line_to(x1-arrow_width, y1-arrow_width)\n",
    "        c.line_to(x1-arrow_width, y1+arrow_width)\n",
    "        c.line_to(x1, y1)\n",
    "    else:\n",
    "        c.line_to(x1+arrow_width, y1+arrow_width)\n",
    "        c.line_to(x1-arrow_width, y1+arrow_width)\n",
    "        c.line_to(x1, y1)\n",
    "    c.fill()\n",
    "\n",
    "draw_recurrent_node(c, x=60, y=100)\n",
    "draw_recurrent_node(c, x=200, y= 100)\n",
    "draw_arrow(c, 100, 100, 160, 100)\n",
    "draw_arrow(c, 60, 180, 60, 140, direction=\"up\")\n",
    "draw_arrow(c, 200, 180, 200, 140, direction=\"up\")\n",
    "\n",
    "c.font = '12px serif'\n",
    "c.fill_text(\"w0\", 40, 170)\n",
    "c.fill_text(\"w1\", 50, 15)\n",
    "c.fill_text(\"w2\", 120, 90)\n",
    "c.fill_text(\"w3\", 180, 170)\n",
    "c.fill_text(\"w4\", 190, 15)\n",
    "    \n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12544ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0.9325, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.9193, 0.0000]], grad_fn=<AddBackward0>)\n",
      "tensor([[1.2092, 0.8755, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7449, 0.0000, 0.8755, 0.6265, 0.2991]], grad_fn=<AddBackward0>)\n",
      "Error backprop through node 1\n",
      "GS: tensor([[[3.2414]],\n",
      "\n",
      "        [[1.6709]],\n",
      "\n",
      "        [[3.8101]],\n",
      "\n",
      "        [[1.9425]],\n",
      "\n",
      "        [[2.5330]]], grad_fn=<ViewBackward0>)\n",
      "Actual: tensor([[[3.2414]],\n",
      "\n",
      "        [[1.6709]],\n",
      "\n",
      "        [[3.8101]],\n",
      "\n",
      "        [[1.9425]],\n",
      "\n",
      "        [[2.5330]]])\n",
      "tensor(1.5947, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# gt_w1 = w2 * gt1_w1 + x\n",
    "# gt_w2 = w2 * gt1_w2 + h1\n",
    "\n",
    "def compute_2(xs, hiddens, gs, weights):\n",
    "    y0 = torch.mm(weights[0], xs[0]) + torch.mm(weights[1], hiddens[0])\n",
    "    assert y0 == weights[0,0,0] * xs[0,0,0] + weights[1,0,0] * hiddens[0,0,0]\n",
    "    y1 = torch.mm(weights[3], xs[1]) + torch.mm(weights[4], hiddens[1]) + torch.mm(weights[2], hiddens[0])\n",
    "    assert y1 == weights[3,0,0] * xs[1,0,0] + weights[4,0,0] * hiddens[1,0,0] + weights[2,0,0] * hiddens[0,0,0]\n",
    "    \n",
    "    # [[ dh0/dh0_t, dh0/dh1_t ]\n",
    "    #  [ dh1/dh0_t, dh1/dh1_t ]]\n",
    "    Ht = torch.zeros((hiddens.shape[0], hiddens.shape[0]))\n",
    "    Ht[0,0] = weights[1,0,0]\n",
    "    Ht[0,1] = 0\n",
    "    Ht[1,0] = weights[2,0,0]\n",
    "    Ht[1,1] = weights[4,0,0]\n",
    "    \n",
    "    # [[ dh0/dw0 dh0/dw1 dh0/dw2 dh0/dw3 dh0/dw4]\n",
    "    #  [ dh1/dw0 dh1/dw1 dh1/dw2 dh1/dw3 dh1/dw4]]\n",
    "    Ft = torch.zeros((hiddens.shape[0], 5))\n",
    "    Ft[0,0] = xs[0,0,0]\n",
    "    Ft[0,1] = hiddens[0,0,0]\n",
    "    \n",
    "    # What are these?\n",
    "    Ft[1,0] = 0\n",
    "    Ft[1,1] = 0\n",
    "    Ft[1,2] = hiddens[0,0,0]\n",
    "    \n",
    "    Ft[1,3] = xs[1,0,0]\n",
    "    Ft[1,4] = hiddens[1,0,0] \n",
    "    \n",
    "    gs2 = torch.mm(Ht, gs) + Ft\n",
    "    #gs2[:] = 0\n",
    "    #gs2[0,0] = weights[1,0,0] * gs[0,0] + xs[0,0,0]\n",
    "    \n",
    "    print(gs)\n",
    "    #torch.testing.assert_allclose(gs_p, gs)\n",
    "    \n",
    "    y = torch.stack((y0, y1), dim=0)\n",
    "    \n",
    "    assert y.shape == hiddens.shape\n",
    "    \n",
    "    return (y, gs2)\n",
    "    \n",
    "\n",
    "def forward_grad_2node():\n",
    "    num_nodes = 2\n",
    "    weights = torch.rand((5, 1, 1), requires_grad=True)\n",
    "    # two inputs at t=0\n",
    "    x0 = torch.rand((num_nodes, 1, 1), requires_grad=True)\n",
    "    hiddens = torch.zeros((num_nodes, 1, 1), requires_grad=True)\n",
    "    gs = torch.zeros((num_nodes, 5), requires_grad=False)\n",
    "    \n",
    "    (hiddens, gs) = compute_2(x0, hiddens, gs, weights)\n",
    "    torch.testing.assert_allclose(hiddens, torch.tensor([\n",
    "        [[ x0[0,0,0] * weights[0,0,0] ]],\n",
    "        [[ x0[1,0,0] * weights[3,0,0] ]]\n",
    "    ]))\n",
    "    \n",
    "    if True:\n",
    "        x1 = torch.rand((num_nodes, 1, 1), requires_grad=True)\n",
    "        (hiddens, gs) = compute_2(x1, hiddens, gs, weights)\n",
    "    \n",
    "        if True:\n",
    "            x2 = torch.rand((num_nodes, 1, 1), requires_grad=True)\n",
    "            (hiddens, gs) = compute_2(x2, hiddens, gs, weights)\n",
    "    \n",
    "    \n",
    "    y_actual = torch.rand(2,1,1)\n",
    "    assert y_actual.shape == hiddens.shape\n",
    "    if False:\n",
    "        error = torch.nn.MSELoss()(hiddens[0], y_actual[0])\n",
    "        error.backward()\n",
    "        error_grad = 2 * (hiddens[0] - y_actual[0])\n",
    "        gs_grad = (gs * error_grad)[0].view(5,1,1)\n",
    "        print(\"Error backprop through node 0\")\n",
    "        print(\"GS:\", gs_grad)\n",
    "        print(\"Actual:\", weights.grad)\n",
    "        torch.testing.assert_allclose(gs_grad, weights.grad)\n",
    "    \n",
    "    elif True:\n",
    "        error = torch.nn.MSELoss()(hiddens[1], y_actual[1])\n",
    "        error.backward()\n",
    "        error_grad = 2 * (hiddens[1] - y_actual[1])[0,0]\n",
    "        gs_grad = (gs * error_grad)[1].view(5,1,1)\n",
    "        print(\"Error backprop through node 1\")\n",
    "        print(\"GS:\", gs_grad)\n",
    "        print(\"Actual:\", weights.grad)\n",
    "        print(weights.grad[2,0,0]/error_grad)\n",
    "        torch.testing.assert_allclose(gs_grad, weights.grad)\n",
    "        \n",
    "    else:\n",
    "        error = torch.nn.MSELoss()(hiddens, y_actual)\n",
    "        error.backward()\n",
    "        error_grad = 2 * (hiddens - y_actual) / 2\n",
    "        print(error_grad)\n",
    "        gs_grad = (gs * error_grad)\n",
    "        print(\"Error backprop through both\")\n",
    "        print(\"GS:\", gs_grad)\n",
    "        print(\"Actual:\", weights.grad)\n",
    "        #torch.testing.assert_allclose(gs_grad, weights.grad)\n",
    "    \n",
    "    \n",
    "forward_grad_2node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274b569d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0420, 0.1882, 1.0548, 1.4992, 1.1743],\n",
       "        [1.2900, 0.8947, 0.7786, 1.0508, 1.3800],\n",
       "        [1.3108, 1.4179, 1.3641, 0.8876, 0.9686]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,3,5).sum(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0074a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3ddc8d2",
   "metadata": {},
   "source": [
    "    model_clone = copy.deepcopy(model)\n",
    "    optimizer = torch.optim.SGD(model_clone.parameters(), 0.) #used for zero.grad() function only here\n",
    "    T = x.shape[0]    \n",
    "    theta = torch.nn.utils.convert_parameters.parameters_to_vector(model_clone.parameters())      \n",
    "    n_params = len(theta)\n",
    "    \n",
    "    h = torch.randn(model.hidden_size, dtype=model.dtype, requires_grad=True)\n",
    "    dh_dtheta = torch.zeros((model.hidden_size, n_params), dtype=model.dtype) \n",
    "    dhnext_dhprev = torch.zeros((model.hidden_size, model.hidden_size), dtype=model.dtype)\n",
    "    partial_dh_dtheta = torch.zeros_like(dh_dtheta)\n",
    "    \n",
    "    for t in range(T): \n",
    "        h_next = model_clone.h_step(x[t].view(1,1), h.view(1, model.hidden_size)).view(model.hidden_size)\n",
    "        #compute dh/dhprev and partial dh/dparams\n",
    "        for i_h in range(model.hidden_size):\n",
    "            v = torch.zeros(model.hidden_size, dtype=model.dtype)\n",
    "            v[i_h] = 1.  \n",
    "            if i_h == model.hidden_size-1:\n",
    "                h_next.backward(v) \n",
    "            else:\n",
    "                h_next.backward(v, retain_graph=True) \n",
    "            dhnext_dhprev[i_h] = h.grad.clone()  \n",
    "            h.grad = None             \n",
    "            grad_generator = (param.grad if param.grad is not None else torch.zeros_like(param) for param in model_clone.parameters())                         \n",
    "            theta_grad = torch.nn.utils.convert_parameters.parameters_to_vector(grad_generator)                         \n",
    "            partial_dh_dtheta[i_h] = theta_grad.clone()                \n",
    "            optimizer.zero_grad()        \n",
    "        dh_dtheta = torch.mm(dhnext_dhprev, dh_dtheta) + partial_dh_dtheta                  \n",
    "        h_next = h_next.detach()\n",
    "        h = h_next.clone()            \n",
    "        h.requires_grad = True           \n",
    "   \n",
    "    y_pred = model_clone.h_to_logits(h.view(1, model.hidden_size))\n",
    "    loss = loss_func(y_pred, y.view(1))    \n",
    "    loss.backward()        \n",
    "    #add partial derivative of loss wrt. params and (loss wrt h) times (h wrt params)\n",
    "    grad_generator = (param.grad if param.grad is not None else torch.zeros_like(param) for param in model_clone.parameters())            \n",
    "    partial_theta_grad = torch.nn.utils.convert_parameters.parameters_to_vector(grad_generator)    \n",
    "    theta_grad = partial_theta_grad.clone() + h.grad.clone() @ dh_dtheta   \n",
    "    return loss.item(), theta_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd6d0f3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'h2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1r/bnnycq6n0q17_y55gsn6v_qc0000gn/T/ipykernel_1101/3424091240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmodel_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1r/bnnycq6n0q17_y55gsn6v_qc0000gn/T/ipykernel_1101/3424091240.py\u001b[0m in \u001b[0;36mmodel_example\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'h2'"
     ]
    }
   ],
   "source": [
    "\n",
    "class RNNCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.h2h = torch.nn.Linear(1,1, bias=False)\n",
    "        self.x2h = torch.nn.Linear(1,1, bias=False)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        h = self.h2h(h) + self.x2h(x)\n",
    "        return h\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.r1 = RNNCell()\n",
    "        self.r2 = RNNCell()\n",
    "        \n",
    "    def forward(self, x1, x2, h1, h2,):\n",
    "        h1 = self.r1(x=x, h=h1)\n",
    "        h2 = self.r2(x=h1, h=h2)\n",
    "        return (h1, h2)\n",
    "\n",
    "def model_example():\n",
    "    BS = 1\n",
    "    x0 = torch.rand((BS, 1))\n",
    "    h1 = torch.zeros((1,))\n",
    "    h2 = torch.zeros((1,))\n",
    "    model = Net()\n",
    "    h1, h2 = model(x0, h1, h2)\n",
    "    print(h1, h2)\n",
    "    x1 = torch.rand((BS, 1))\n",
    "    h1, h2 = model(x1, h1, h2)\n",
    "    print(h1, h2)\n",
    "    print([p for p in model.parameters()])\n",
    "    theta = torch.nn.utils.convert_parameters.parameters_to_vector(model.parameters())\n",
    "    print(theta)\n",
    "    \n",
    "model_example()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4e73569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1d79efd54c42c2b3f613f06d6ce02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=300, width=800)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = ipycanvas.Canvas(width=800, height=300)\n",
    "\n",
    "draw_recurrent_node(c, x=60, y=100)\n",
    "draw_recurrent_node(c, x=200, y= 100)\n",
    "draw_arrow(c, 100, 100, 160, 100)\n",
    "draw_arrow(c, x0=60, y0=180, x1=60, y1=140, direction=\"up\")\n",
    "draw_arrow(c, x0=200, y0=180, x1=200, y1=140, direction=\"up\")\n",
    "draw_node(c, x=60, y=220)\n",
    "draw_node(c, x=200, y=220)\n",
    "\n",
    "c.font = '24px serif'\n",
    "c.fill_text(\"0\", 55, 230)\n",
    "c.fill_text(\"1\", 55, 110)\n",
    "c.fill_text(\"2\", 195, 230)\n",
    "c.fill_text(\"3\", 195, 110)\n",
    "    \n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "380b196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1011, grad_fn=<MseLossBackward0>) -0.63604336977005\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-1.4409e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.1500e-04, -1.5699e-04, -0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-4.7575e-01, -0.0000e+00, -5.6732e-03, -5.0792e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.7737e-01, -6.0899e-02, -0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-3.2921e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.4840e-01, -5.4539e-02, -0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-2.1638e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-5.8772e-01, -2.7344e-01, -5.6732e-03, -5.0792e-02]]],\n",
      "       grad_fn=<CopySlices>) tensor([[0.0125],\n",
      "        [0.2980]], grad_fn=<CatBackward0>) tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4757, -0.0000, -0.0057, -0.0508],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2774, -0.0609, -0.0000, -0.0000]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4757, -0.0000, -0.0057, -0.0508],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2774, -0.0609, -0.0000, -0.0000]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4757, -0.0057,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def calc_next_G(G, W, inputs, hiddens):\n",
    "    G_new = torch.zeros_like(G, requires_grad=False)\n",
    "    for k in range(G.shape[0]):\n",
    "        for i in range(G.shape[1]):\n",
    "            for j in range(G.shape[2]):\n",
    "                for p in range(G.shape[0]):\n",
    "                    G_new[k,i,j] += W[k,p] * G[p,i,j]\n",
    "                if i == k:\n",
    "                    if j >= inputs.shape[0]:\n",
    "                        G_new[k,i,j] += hiddens[j-inputs.shape[0], 0]\n",
    "                    else:\n",
    "                        # There is no connection from 2 to 1\n",
    "                        # TODO: other cases\n",
    "                        if i == 1 and j == 1:\n",
    "                            pass\n",
    "                        else:\n",
    "                            G_new[k,i,j] += inputs[j,0]\n",
    "                \n",
    "    return G_new\n",
    "\n",
    "def forward_grad_2hidden_calc(G, W, inputs, hiddens):\n",
    "    new_hiddens = torch.cat((\n",
    "        inputs[0:1] * W[1:2,0:1] + hiddens[0:1] * W[1:2,1:2],\n",
    "        inputs[1:2] * W[3:4,2:3] + hiddens[1:2] * W[3:4,3:4] + hiddens[0:1] * W[3:4,1:2]\n",
    "    ))\n",
    "    \n",
    "    G_new = calc_next_G(G, W, inputs, hiddens)\n",
    "    \n",
    "    return (new_hiddens, G_new)\n",
    "    \n",
    "\n",
    "def forward_grad_2hidden():\n",
    "    num_hidden = 2\n",
    "    num_input = 2\n",
    "    num_nodes = num_hidden + num_input\n",
    "    # Should G be k=num_hidden, i=num_nodes, j=num_hidden?\n",
    "    # Gij^k\n",
    "    G = torch.zeros((num_nodes, num_nodes, num_nodes), requires_grad=True)\n",
    "    W = torch.rand((num_nodes, num_nodes), requires_grad=True)\n",
    "    num_itr = 2\n",
    "    hidden = torch.zeros((num_hidden,1), requires_grad=True)\n",
    "    \n",
    "    for itr in range(num_itr):\n",
    "        inputs = torch.rand((num_input,1))        \n",
    "        hidden, G = forward_grad_2hidden_calc(G, W, inputs, hidden)\n",
    "        \n",
    "    y = torch.rand(2)\n",
    "    error = torch.nn.MSELoss()(hidden[0:1],y[0:1])\n",
    "    error_grad = 2 * (hidden[0:1] - y[0:1])\n",
    "    print(error, error_grad.item())\n",
    "    error.backward()\n",
    "    G_grad = G * error_grad\n",
    "    G_grad[:,0,:] = 0 # No weights to node 0\n",
    "    G_grad[:,2,:] = 0 # No weights to node 2\n",
    "    print(G_grad, hidden, G_grad[1,:,:])\n",
    "    print(G_grad[1,:,:])\n",
    "    print(W.grad)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "forward_grad_2hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f290014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6879, 0.2149], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, requires_grad=True)\n",
    "b = torch.rand(2, requires_grad=True)\n",
    "torch.cat((a[0:1], b[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7a725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
