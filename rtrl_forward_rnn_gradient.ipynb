{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcc7a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ipycanvas\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6231e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2090, -0.3023]) tensor([ 0.0921, -0.0306]) tensor([ 0.2090, -0.3023], grad_fn=<MulBackward0>) tensor([ 0.0921, -0.0306], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def compute(x, h1, h2, gt1_w1, gt1_w2, w1, w2):\n",
    "    # Single recursive node\n",
    "    y = w1 * x + w2 * h1\n",
    "    # dh1/dh2 * dh2/dw + dh1/dw\n",
    "    gt_w1 = w2 * gt1_w1 + x\n",
    "    gt_w2 = w2 * gt1_w2 + h1\n",
    "    return y, gt_w1, gt_w2\n",
    "\n",
    "\n",
    "w1 = torch.rand(2, requires_grad=True)\n",
    "w2 = torch.rand(2, requires_grad=True)\n",
    "h0 = 0\n",
    "x1 = torch.rand(2, requires_grad=True)\n",
    "h1, dh_w1, dh_w2 = compute(x1, h0, 0, 0, 0, w1, w2)\n",
    "#assert h1 == 1\n",
    "\n",
    "x2 = torch.rand(2, requires_grad=True)\n",
    "h2, dh_w1, dh_w2 = compute(x2, h1, h0, dh_w1, dh_w2, w1, w2)\n",
    "#assert h2 == (1.5 + 0.5)\n",
    "\n",
    "x3 = torch.rand(2, requires_grad=True)\n",
    "h3, dh_w1, dh_w2 = compute(x3, h2, h1, dh_w1, dh_w2, w1, w2)\n",
    "\n",
    "y = torch.rand(2)\n",
    "loss = torch.nn.MSELoss()(h3, y)\n",
    "loss.backward()\n",
    "\n",
    "error_grad = 2 * (h3 - y) / 2\n",
    "# YAY! gt1_w1 is correct! With gt_w1 = w2 * gt1_w1 + x\n",
    "# YAY! gt1_w2 is correct! With gt_w2 = w2 * gt1_w2 + h1\n",
    "print(w1.grad, w2.grad, dh_w1*error_grad, dh_w2*error_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46bd3a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4fccb6a7f14ca59d26002207e1246c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=200, width=800)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "c = ipycanvas.Canvas(width=800, height=200)\n",
    "\n",
    "\n",
    "def draw_recurrent_node(c, x=60, y=100):\n",
    "\n",
    "    c.stroke_style = \"red\"\n",
    "    c.stroke_circle(x, y, 40)\n",
    "\n",
    "    c.begin_path()\n",
    "    y_arc = y - 30\n",
    "    x0 = x+20\n",
    "    x1 = x-20\n",
    "    c.move_to(x0, y_arc)\n",
    "    c.quadratic_curve_to(x0 + 5, y_arc-50, x0 + (x1 - x0)//2, y_arc - 50)\n",
    "    c.quadratic_curve_to(x1 - 5, y_arc-50, x1, y_arc)\n",
    "    c.stroke()\n",
    "    c.begin_path()\n",
    "    arrow_width = 5\n",
    "    c.line_to(x1-arrow_width, y_arc)\n",
    "    c.line_to(x1, y_arc+arrow_width)\n",
    "    c.line_to(x1+arrow_width, y_arc)\n",
    "    c.fill()\n",
    "    \n",
    "def draw_arrow(c, x0, y0, x1, y1, arrow_width=5, direction=\"right\"):\n",
    "    c.begin_path()\n",
    "    c.move_to(x0, y0)\n",
    "    c.line_to(x1, y1)\n",
    "    c.stroke()\n",
    "    c.begin_path()\n",
    "    if direction == \"right\":\n",
    "        c.line_to(x1-arrow_width, y1-arrow_width)\n",
    "        c.line_to(x1-arrow_width, y1+arrow_width)\n",
    "        c.line_to(x1, y1)\n",
    "    else:\n",
    "        c.line_to(x1+arrow_width, y1+arrow_width)\n",
    "        c.line_to(x1-arrow_width, y1+arrow_width)\n",
    "        c.line_to(x1, y1)\n",
    "    c.fill()\n",
    "\n",
    "draw_recurrent_node(c, x=60, y=100)\n",
    "draw_recurrent_node(c, x=200, y= 100)\n",
    "draw_arrow(c, 100, 100, 160, 100)\n",
    "draw_arrow(c, 60, 180, 60, 140, direction=\"up\")\n",
    "draw_arrow(c, 200, 180, 200, 140, direction=\"up\")\n",
    "\n",
    "c.font = '12px serif'\n",
    "c.fill_text(\"w0\", 40, 170)\n",
    "c.fill_text(\"w1\", 50, 15)\n",
    "c.fill_text(\"w2\", 120, 90)\n",
    "c.fill_text(\"w3\", 180, 170)\n",
    "c.fill_text(\"w4\", 190, 15)\n",
    "    \n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b12544ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0.9092, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2666, 0.0000]], grad_fn=<AddBackward0>)\n",
      "tensor([[0.4751, 0.7062, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8663, 0.0000, 0.7062, 1.0641, 0.0051]], grad_fn=<AddBackward0>)\n",
      "Error backprop through node 1\n",
      "GS: tensor([[[0.3818]],\n",
      "\n",
      "        [[0.3714]],\n",
      "\n",
      "        [[0.3113]],\n",
      "\n",
      "        [[0.1893]],\n",
      "\n",
      "        [[0.3834]]], grad_fn=<ViewBackward0>)\n",
      "Actual: tensor([[[0.3818]],\n",
      "\n",
      "        [[0.3714]],\n",
      "\n",
      "        [[0.3113]],\n",
      "\n",
      "        [[0.1893]],\n",
      "\n",
      "        [[0.3834]]])\n",
      "tensor(0.5639, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# gt_w1 = w2 * gt1_w1 + x\n",
    "# gt_w2 = w2 * gt1_w2 + h1\n",
    "\n",
    "def compute_2(xs, hiddens, gs, weights):\n",
    "    y0 = torch.mm(weights[0], xs[0]) + torch.mm(weights[1], hiddens[0])\n",
    "    assert y0 == weights[0,0,0] * xs[0,0,0] + weights[1,0,0] * hiddens[0,0,0]\n",
    "    y1 = torch.mm(weights[3], xs[1]) + torch.mm(weights[4], hiddens[1]) + torch.mm(weights[2], hiddens[0])\n",
    "    assert y1 == weights[3,0,0] * xs[1,0,0] + weights[4,0,0] * hiddens[1,0,0] + weights[2,0,0] * hiddens[0,0,0]\n",
    "    \n",
    "    # [[ dh0/dh0_t, dh0/dh1_t ]\n",
    "    #  [ dh1/dh0_t, dh1/dh1_t ]]\n",
    "    Ht = torch.zeros((hiddens.shape[0], hiddens.shape[0]))\n",
    "    Ht[0,0] = weights[1,0,0]\n",
    "    Ht[0,1] = 0\n",
    "    Ht[1,0] = weights[2,0,0]\n",
    "    Ht[1,1] = weights[4,0,0]\n",
    "    \n",
    "    # [[ dh0/dw0 dh0/dw1 dh0/dw2 dh0/dw3 dh0/dw4]\n",
    "    #  [ dh1/dw0 dh1/dw1 dh1/dw2 dh1/dw3 dh1/dw4]]\n",
    "    Ft = torch.zeros((hiddens.shape[0], 5))\n",
    "    Ft[0,0] = xs[0,0,0]\n",
    "    Ft[0,1] = hiddens[0,0,0]\n",
    "    \n",
    "    # What are these?\n",
    "    Ft[1,0] = 0\n",
    "    Ft[1,1] = 0\n",
    "    Ft[1,2] = hiddens[0,0,0]\n",
    "    \n",
    "    Ft[1,3] = xs[1,0,0]\n",
    "    Ft[1,4] = hiddens[1,0,0] \n",
    "    \n",
    "    gs2 = torch.mm(Ht, gs) + Ft\n",
    "    #gs2[:] = 0\n",
    "    #gs2[0,0] = weights[1,0,0] * gs[0,0] + xs[0,0,0]\n",
    "    \n",
    "    print(gs)\n",
    "    #torch.testing.assert_allclose(gs_p, gs)\n",
    "    \n",
    "    y = torch.stack((y0, y1), dim=0)\n",
    "    \n",
    "    assert y.shape == hiddens.shape\n",
    "    \n",
    "    return (y, gs2)\n",
    "    \n",
    "\n",
    "def forward_grad_2node():\n",
    "    num_nodes = 2\n",
    "    weights = torch.rand((5, 1, 1), requires_grad=True)\n",
    "    # two inputs at t=0\n",
    "    x0 = torch.rand((num_nodes, 1, 1), requires_grad=True)\n",
    "    hiddens = torch.zeros((num_nodes, 1, 1), requires_grad=True)\n",
    "    gs = torch.zeros((num_nodes, 5), requires_grad=False)\n",
    "    \n",
    "    (hiddens, gs) = compute_2(x0, hiddens, gs, weights)\n",
    "    torch.testing.assert_allclose(hiddens, torch.tensor([\n",
    "        [[ x0[0,0,0] * weights[0,0,0] ]],\n",
    "        [[ x0[1,0,0] * weights[3,0,0] ]]\n",
    "    ]))\n",
    "    \n",
    "    if True:\n",
    "        x1 = torch.rand((num_nodes, 1, 1), requires_grad=True)\n",
    "        (hiddens, gs) = compute_2(x1, hiddens, gs, weights)\n",
    "    \n",
    "        if True:\n",
    "            x2 = torch.rand((num_nodes, 1, 1), requires_grad=True)\n",
    "            (hiddens, gs) = compute_2(x2, hiddens, gs, weights)\n",
    "    \n",
    "    \n",
    "    y_actual = torch.rand(2,1,1)\n",
    "    assert y_actual.shape == hiddens.shape\n",
    "    if False:\n",
    "        error = torch.nn.MSELoss()(hiddens[0], y_actual[0])\n",
    "        error.backward()\n",
    "        error_grad = 2 * (hiddens[0] - y_actual[0])\n",
    "        gs_grad = (gs * error_grad)[0].view(5,1,1)\n",
    "        print(\"Error backprop through node 0\")\n",
    "        print(\"GS:\", gs_grad)\n",
    "        print(\"Actual:\", weights.grad)\n",
    "        torch.testing.assert_allclose(gs_grad, weights.grad)\n",
    "    \n",
    "    elif True:\n",
    "        error = torch.nn.MSELoss()(hiddens[1], y_actual[1])\n",
    "        error.backward()\n",
    "        error_grad = 2 * (hiddens[1] - y_actual[1])[0,0]\n",
    "        gs_grad = (gs * error_grad)[1].view(5,1,1)\n",
    "        print(\"Error backprop through node 1\")\n",
    "        print(\"GS:\", gs_grad)\n",
    "        print(\"Actual:\", weights.grad)\n",
    "        print(weights.grad[2,0,0]/error_grad)\n",
    "        torch.testing.assert_allclose(gs_grad, weights.grad)\n",
    "        \n",
    "    else:\n",
    "        error = torch.nn.MSELoss()(hiddens, y_actual)\n",
    "        error.backward()\n",
    "        error_grad = 2 * (hiddens - y_actual) / 2\n",
    "        print(error_grad)\n",
    "        gs_grad = (gs * error_grad)\n",
    "        print(\"Error backprop through both\")\n",
    "        print(\"GS:\", gs_grad)\n",
    "        print(\"Actual:\", weights.grad)\n",
    "        #torch.testing.assert_allclose(gs_grad, weights.grad)\n",
    "    \n",
    "    \n",
    "forward_grad_2node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "274b569d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6085"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,3,5).sum(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0074a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3ddc8d2",
   "metadata": {},
   "source": [
    "    model_clone = copy.deepcopy(model)\n",
    "    optimizer = torch.optim.SGD(model_clone.parameters(), 0.) #used for zero.grad() function only here\n",
    "    T = x.shape[0]    \n",
    "    theta = torch.nn.utils.convert_parameters.parameters_to_vector(model_clone.parameters())      \n",
    "    n_params = len(theta)\n",
    "    \n",
    "    h = torch.randn(model.hidden_size, dtype=model.dtype, requires_grad=True)\n",
    "    dh_dtheta = torch.zeros((model.hidden_size, n_params), dtype=model.dtype) \n",
    "    dhnext_dhprev = torch.zeros((model.hidden_size, model.hidden_size), dtype=model.dtype)\n",
    "    partial_dh_dtheta = torch.zeros_like(dh_dtheta)\n",
    "    \n",
    "    for t in range(T): \n",
    "        h_next = model_clone.h_step(x[t].view(1,1), h.view(1, model.hidden_size)).view(model.hidden_size)\n",
    "        #compute dh/dhprev and partial dh/dparams\n",
    "        for i_h in range(model.hidden_size):\n",
    "            v = torch.zeros(model.hidden_size, dtype=model.dtype)\n",
    "            v[i_h] = 1.  \n",
    "            if i_h == model.hidden_size-1:\n",
    "                h_next.backward(v) \n",
    "            else:\n",
    "                h_next.backward(v, retain_graph=True) \n",
    "            dhnext_dhprev[i_h] = h.grad.clone()  \n",
    "            h.grad = None             \n",
    "            grad_generator = (param.grad if param.grad is not None else torch.zeros_like(param) for param in model_clone.parameters())                         \n",
    "            theta_grad = torch.nn.utils.convert_parameters.parameters_to_vector(grad_generator)                         \n",
    "            partial_dh_dtheta[i_h] = theta_grad.clone()                \n",
    "            optimizer.zero_grad()        \n",
    "        dh_dtheta = torch.mm(dhnext_dhprev, dh_dtheta) + partial_dh_dtheta                  \n",
    "        h_next = h_next.detach()\n",
    "        h = h_next.clone()            \n",
    "        h.requires_grad = True           \n",
    "   \n",
    "    y_pred = model_clone.h_to_logits(h.view(1, model.hidden_size))\n",
    "    loss = loss_func(y_pred, y.view(1))    \n",
    "    loss.backward()        \n",
    "    #add partial derivative of loss wrt. params and (loss wrt h) times (h wrt params)\n",
    "    grad_generator = (param.grad if param.grad is not None else torch.zeros_like(param) for param in model_clone.parameters())            \n",
    "    partial_theta_grad = torch.nn.utils.convert_parameters.parameters_to_vector(grad_generator)    \n",
    "    theta_grad = partial_theta_grad.clone() + h.grad.clone() @ dh_dtheta   \n",
    "    return loss.item(), theta_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd6d0f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1369]], grad_fn=<AddBackward0>) tensor([[-0.0677]], grad_fn=<AddBackward0>)\n",
      "tensor([[0.1197]], grad_fn=<AddBackward0>) tensor([[0.1040]], grad_fn=<AddBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[-0.8895]], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1743]], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.6627]], requires_grad=True), Parameter containing:\n",
      "tensor([[0.4943]], requires_grad=True)]\n",
      "tensor([-0.8895, -0.1743, -0.6627,  0.4943], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RNNCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.h2h = torch.nn.Linear(1,1, bias=False)\n",
    "        self.x2h = torch.nn.Linear(1,1, bias=False)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        h = self.h2h(h) + self.x2h(x)\n",
    "        return h\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.r1 = RNNCell()\n",
    "        self.r2 = RNNCell()\n",
    "        \n",
    "    def forward(self, x1, x2, h1, h2,):\n",
    "        h1 = self.r1(x=x, h=h1)\n",
    "        h2 = self.r2(x=h1, h=h2)\n",
    "        return (h1, h2)\n",
    "\n",
    "def model_example():\n",
    "    BS = 1\n",
    "    x0 = torch.rand((BS, 1))\n",
    "    h1 = torch.zeros((1,))\n",
    "    h2 = torch.zeros((1,))\n",
    "    model = Net()\n",
    "    h1, h2 = model(x0, h1, h2)\n",
    "    print(h1, h2)\n",
    "    x1 = torch.rand((BS, 1))\n",
    "    h1, h2 = model(x1, h1, h2)\n",
    "    print(h1, h2)\n",
    "    print([p for p in model.parameters()])\n",
    "    theta = torch.nn.utils.convert_parameters.parameters_to_vector(model.parameters())\n",
    "    print(theta)\n",
    "    \n",
    "model_example()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e73569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
